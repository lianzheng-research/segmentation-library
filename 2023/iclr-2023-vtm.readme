# 123
VTM is the first universal few-shot learner for arbitrary dense prediction task. It employs non-parametric matching on patch-level embedded tokens of images and labels within a hierarchical encoder-decoder architecture that is optimized with the standard episodic meta-learning protocol.
